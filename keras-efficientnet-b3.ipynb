{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christian/anaconda3/envs/keras/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/christian/anaconda3/envs/keras/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/christian/anaconda3/envs/keras/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/christian/anaconda3/envs/keras/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/christian/anaconda3/envs/keras/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/christian/anaconda3/envs/keras/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/christian/anaconda3/envs/keras/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/christian/anaconda3/envs/keras/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/christian/anaconda3/envs/keras/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/christian/anaconda3/envs/keras/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/christian/anaconda3/envs/keras/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/christian/anaconda3/envs/keras/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "import collections\n",
    "import sys\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import multiprocessing\n",
    "\n",
    "from math import ceil, floor\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.models import Model, load_model\n",
    "from keras.utils import Sequence\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install and import the efficientnet and iterative-stratification packages from the internet. The iterative-stratification package provides a very nice implementation of multi-label stratification. I've used it in a few competitions now with good results. There are offcourse more packages that provide implementations for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Modules from internet\n",
    "#!pip install efficientnet\n",
    "#!pip install iterative-stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Custom Modules\n",
    "import efficientnet.keras as efn \n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will set the random_state, some constants and folders that will be used later on. I've specified a rather small test size as I want to maximize the training time available and minimize the time used for validation. I'am not using methods like early stopping...when the kernel time limit is approaching we could still increase the results on the LB if we were allowed to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Seed\n",
    "SEED = 12345\n",
    "np.random.seed(SEED)\n",
    "tf.set_random_seed(SEED)\n",
    "\n",
    "# Constants\n",
    "TEST_SIZE = 0.01\n",
    "HEIGHT = 256\n",
    "WIDTH = 256\n",
    "CHANNELS = 3\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 64\n",
    "SHAPE = (HEIGHT, WIDTH, CHANNELS)\n",
    "\n",
    "# Folders\n",
    "DATA_DIR = 'data/'\n",
    "TEST_IMAGES_DIR = DATA_DIR + 'stage_1_test_images/'\n",
    "TRAIN_IMAGES_DIR = DATA_DIR + 'stage_1_train_images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the code for the DICOM windowing and the Data Generators. After seeing the effect of different versions of windowing as presented in this very nice [kernel](https://www.kaggle.com/akensert/inceptionv3-prev-resnet50-keras-baseline-model) I decided to also update my kernel with it. Lets see what the effect will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_dcm(dcm):\n",
    "    x = dcm.pixel_array + 1000\n",
    "    px_mode = 4096\n",
    "    x[x>=px_mode] = x[x>=px_mode] - px_mode\n",
    "    dcm.PixelData = x.tobytes()\n",
    "    dcm.RescaleIntercept = -1000\n",
    "\n",
    "def window_image(dcm, window_center, window_width):    \n",
    "    if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100):\n",
    "        correct_dcm(dcm)\n",
    "    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n",
    "    \n",
    "    # Resize\n",
    "    img = cv2.resize(img, SHAPE[:2], interpolation = cv2.INTER_LINEAR)\n",
    "   \n",
    "    img_min = window_center - window_width // 2\n",
    "    img_max = window_center + window_width // 2\n",
    "    img = np.clip(img, img_min, img_max)\n",
    "    return img\n",
    "\n",
    "def bsb_window(dcm):\n",
    "    brain_img = window_image(dcm, 40, 80)\n",
    "    subdural_img = window_image(dcm, 80, 200)\n",
    "    soft_img = window_image(dcm, 40, 380)\n",
    "    \n",
    "    brain_img = (brain_img - 0) / 80\n",
    "    subdural_img = (subdural_img - (-20)) / 200\n",
    "    soft_img = (soft_img - (-150)) / 380\n",
    "    bsb_img = np.array([brain_img, subdural_img, soft_img]).transpose(1,2,0)\n",
    "    return bsb_img\n",
    "\n",
    "def _read(path, SHAPE):\n",
    "    dcm = pydicom.dcmread(path)\n",
    "    try:\n",
    "        img = bsb_window(dcm)\n",
    "    except:\n",
    "        img = np.zeros(SHAPE)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll specify some light image augmentation. Some horizontal and vertical flipping and some cropping. I haven't yet tried out more augmentation but will do so in future versions of the kernel. Also the code for Data Generators for train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Augmentation\n",
    "sometimes = lambda aug: iaa.Sometimes(0.25, aug)\n",
    "augmentation = iaa.Sequential([ iaa.Fliplr(0.25),\n",
    "                                iaa.Flipud(0.10),\n",
    "                                sometimes(iaa.Crop(px=(0, 25), keep_size = True, sample_independently = False))   \n",
    "                            ], random_order = True)       \n",
    "        \n",
    "# Generators\n",
    "class TrainDataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, dataset, labels, batch_size = 16, img_size = SHAPE, img_dir = TRAIN_IMAGES_DIR, augment = False, *args, **kwargs):\n",
    "        self.dataset = dataset\n",
    "        self.ids = dataset.index\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.img_dir = img_dir\n",
    "        self.augment = augment\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(ceil(len(self.ids) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        X, Y = self.__data_generation(indices)\n",
    "        return X, Y\n",
    "\n",
    "    def augmentor(self, image):\n",
    "        augment_img = augmentation        \n",
    "        image_aug = augment_img.augment_image(image)\n",
    "        return image_aug\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(len(self.ids))\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "    def __data_generation(self, indices):\n",
    "        X = np.empty((self.batch_size, *self.img_size))\n",
    "        Y = np.empty((self.batch_size, 6), dtype=np.float32)\n",
    "        \n",
    "        for i, index in enumerate(indices):\n",
    "            ID = self.ids[index]\n",
    "            image = _read(self.img_dir+ID+\".dcm\", self.img_size)\n",
    "            if self.augment:\n",
    "                X[i,] = self.augmentor(image)\n",
    "            else:\n",
    "                X[i,] = image\n",
    "            Y[i,] = self.labels.iloc[index].values        \n",
    "        return X, Y\n",
    "    \n",
    "class TestDataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, dataset, labels, batch_size = 16, img_size = SHAPE, img_dir = TEST_IMAGES_DIR, *args, **kwargs):\n",
    "        self.dataset = dataset\n",
    "        self.ids = dataset.index\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.img_dir = img_dir\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(ceil(len(self.ids) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        X = self.__data_generation(indices)\n",
    "        return X\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(len(self.ids))\n",
    "    \n",
    "    def __data_generation(self, indices):\n",
    "        X = np.empty((self.batch_size, *self.img_size))\n",
    "        \n",
    "        for i, index in enumerate(indices):\n",
    "            ID = self.ids[index]\n",
    "            image = _read(self.img_dir+ID+\".dcm\", self.img_size)\n",
    "            X[i,] = image              \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_testset(filename = DATA_DIR + \"stage_1_sample_submission.csv\"):\n",
    "    df = pd.read_csv(filename)\n",
    "    df[\"Image\"] = df[\"ID\"].str.slice(stop=12)\n",
    "    df[\"Diagnosis\"] = df[\"ID\"].str.slice(start=13)\n",
    "    df = df.loc[:, [\"Label\", \"Diagnosis\", \"Image\"]]\n",
    "    df = df.set_index(['Image', 'Diagnosis']).unstack(level=-1)\n",
    "    return df\n",
    "\n",
    "def read_trainset(filename = DATA_DIR + \"stage_1_train.csv\"):\n",
    "    df = pd.read_csv(filename)\n",
    "    df[\"Image\"] = df[\"ID\"].str.slice(stop=12)\n",
    "    df[\"Diagnosis\"] = df[\"ID\"].str.slice(start=13)\n",
    "    duplicates_to_remove = [\n",
    "        1598538, 1598539, 1598540, 1598541, 1598542, 1598543,\n",
    "        312468,  312469,  312470,  312471,  312472,  312473,\n",
    "        2708700, 2708701, 2708702, 2708703, 2708704, 2708705,\n",
    "        3032994, 3032995, 3032996, 3032997, 3032998, 3032999\n",
    "    ]\n",
    "    df = df.drop(index = duplicates_to_remove)\n",
    "    df = df.reset_index(drop = True)    \n",
    "    df = df.loc[:, [\"Label\", \"Diagnosis\", \"Image\"]]\n",
    "    df = df.set_index(['Image', 'Diagnosis']).unstack(level=-1)\n",
    "    return df\n",
    "\n",
    "# Read Train and Test Datasets\n",
    "test_df = read_testset()\n",
    "train_df = read_trainset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data contains some class inbalance. Multiple kernels explored the use of undersampling..so let's try the opposite and oversample the minority class 'epidural' one additional time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (677019, 6)\n",
      "Test Shape: (78545, 6)\n"
     ]
    }
   ],
   "source": [
    "# Oversampling\n",
    "epidural_df = train_df[train_df.Label['epidural'] == 1]\n",
    "train_oversample_df = pd.concat([train_df, epidural_df])\n",
    "train_df = train_oversample_df\n",
    "\n",
    "# Summary\n",
    "print('Train Shape: {}'.format(train_df.shape))\n",
    "print('Test Shape: {}'.format(test_df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some methods for predictions on the test data, a callback method and a method to create the EfficientNet B2 model. For the EfficientNet we use the pretrained imagenet weights. Also a Dropout layer is added with a small value to prevent some overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(test_df, model):    \n",
    "    test_preds = model.predict_generator(TestDataGenerator(test_df, None, 5, SHAPE, TEST_IMAGES_DIR), verbose = 1)\n",
    "    return test_preds[:test_df.iloc[range(test_df.shape[0])].shape[0]]\n",
    "\n",
    "def ModelCheckpointFull(model_name):\n",
    "    return ModelCheckpoint(model_name, \n",
    "                            monitor = 'val_loss', \n",
    "                            verbose = 1, \n",
    "                            save_best_only = False, \n",
    "                            save_weights_only = True, \n",
    "                            mode = 'min', \n",
    "                            period = 1)\n",
    "\n",
    "# Create Model\n",
    "def create_model():\n",
    "    K.clear_session()\n",
    "    \n",
    "    base_model =  efn.EfficientNetB3(weights = 'imagenet', include_top = False, pooling = 'avg', input_shape = SHAPE)\n",
    "    x = base_model.output\n",
    "    # Lets try without Dropout\n",
    "    # x = Dropout(0.125)(x)\n",
    "    y_pred = Dense(6, activation = 'sigmoid')(x)\n",
    "\n",
    "    return Model(inputs = base_model.input, outputs = y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we setup the multi label stratification. I've specified multiple splits but only using the first one for train data and validation data. Optionally you can also loop through the different splits and use a different train and validation set for each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission Placeholder\n",
    "submission_predictions = []\n",
    "\n",
    "# Multi Label Stratified Split stuff...\n",
    "msss = MultilabelStratifiedShuffleSplit(n_splits = 10, test_size = TEST_SIZE, random_state = SEED)\n",
    "X = train_df.index\n",
    "Y = train_df.Label.values\n",
    "\n",
    "# Get train and test index\n",
    "msss_splits = next(msss.split(X, Y))\n",
    "train_idx = msss_splits[0]\n",
    "valid_idx = msss_splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the model for a number of epochs. All epochs we train the full model but each time on only 1/6 of the train data. With each epoch only a subset of the train data will allow us to make more epochs and allows todo averaging over more then just 1 or 2 epochs (compared to using all data every epoch).\n",
    "\n",
    "Note that I recreate the data generators and model on each epoch. This is only necessary when using the different Multi-label stratified splits since the data generators will get a totally different set of data on each epoch then. I left it in so that you can try it out.\n",
    "\n",
    "Starting with the 6th epoch a prediction for the test set is made on each epoch. In total predictions from the last 6 epochs will be averaged this way for the final submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through Folds of Multi Label Stratified Split\n",
    "#for epoch, msss_splits in zip(range(0, 9), msss.split(X, Y)): \n",
    "#    # Get train and test index\n",
    "#    train_idx = msss_splits[0]\n",
    "#    valid_idx = msss_splits[1]\n",
    "#for epoch in range(0, 11):\n",
    "#    print('=========== EPOCH {}'.format(epoch))\n",
    "#\n",
    "#    # Shuffle Train data\n",
    "#    np.random.shuffle(train_idx)\n",
    "#    print(train_idx[:5])    \n",
    "#    print(valid_idx[:5])\n",
    "#\n",
    "#    # Create Data Generators for Train and Valid\n",
    "#    data_generator_train = TrainDataGenerator(train_df.iloc[train_idx], \n",
    "#                                                train_df.iloc[train_idx], \n",
    "#                                                TRAIN_BATCH_SIZE, \n",
    "#                                                SHAPE,\n",
    "#                                                augment = True)\n",
    "#    data_generator_val = TrainDataGenerator(train_df.iloc[valid_idx], \n",
    "#                                            train_df.iloc[valid_idx], \n",
    "#                                            VALID_BATCH_SIZE, \n",
    "#                                            SHAPE,\n",
    "#                                            augment = False)\n",
    "#\n",
    "#    # Create Model\n",
    "#    model = create_model()\n",
    "#    \n",
    "#    # Full Training Model\n",
    "#    for base_layer in model.layers[:-1]:\n",
    "#        base_layer.trainable = True\n",
    "#    TRAIN_STEPS = int(len(data_generator_train) / 6)\n",
    "#    LR = 0.00011\n",
    "#\n",
    "#    if epoch != 0:\n",
    "#        # Load Model Weights\n",
    "#        model.load_weights('model.h5')    \n",
    "#\n",
    "#    model.compile(optimizer = Adam(learning_rate = LR), \n",
    "#                  loss = 'binary_crossentropy',\n",
    "#                  metrics = ['acc', tf.keras.metrics.AUC()])\n",
    "#    \n",
    "#    # Train Model\n",
    "#    model.fit_generator(generator = data_generator_train,\n",
    "#                        validation_data = data_generator_val,\n",
    "#                        steps_per_epoch = TRAIN_STEPS,\n",
    "#                        epochs = 1,\n",
    "#                        callbacks = [ModelCheckpointFull('model.h5')],\n",
    "#                        verbose = 1)\n",
    "#    \n",
    "#    # Starting with the 6th epoch we create predictions for the test set on each epoch\n",
    "#    if epoch >= 5:\n",
    "#        preds = predictions(test_df, model)\n",
    "#        submission_predictions.append(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we create the submission file by averaging all submission_predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.load_weights('data/saved_models/efficientnet2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Shape: (78545, 6)\n"
     ]
    }
   ],
   "source": [
    "test_df = read_testset()\n",
    "print('Test Shape: {}'.format(test_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/christian/anaconda3/envs/keras/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "15709/15709 [==============================] - 4878s 311ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = predictions(test_df, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78545, 6)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_flat = preds.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "labels = []\n",
    "\n",
    "for idx,pred in zip(test_df.index, preds):\n",
    "    for i,label in enumerate(test_df.Label):\n",
    "        ids.append(f\"{idx}_{label}\")\n",
    "        predicted_probability = '{0:1.10f}'.format(pred[i].item())\n",
    "        labels.append(predicted_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_000012eaf_any</td>\n",
       "      <td>0.0245890021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_000012eaf_epidural</td>\n",
       "      <td>0.0003140867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_000012eaf_intraparenchymal</td>\n",
       "      <td>0.0054492056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_000012eaf_intraventricular</td>\n",
       "      <td>0.0001541376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_000012eaf_subarachnoid</td>\n",
       "      <td>0.0026069283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              ID         Label\n",
       "0               ID_000012eaf_any  0.0245890021\n",
       "1          ID_000012eaf_epidural  0.0003140867\n",
       "2  ID_000012eaf_intraparenchymal  0.0054492056\n",
       "3  ID_000012eaf_intraventricular  0.0001541376\n",
       "4      ID_000012eaf_subarachnoid  0.0026069283"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_csv = pd.DataFrame({'ID': ids, 'Label': labels})\n",
    "df_csv.to_csv(f'submission_efficientnet2.csv', index=False)\n",
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
